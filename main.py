import os
os.environ['LLAMA_SET_ROWS'] = '1'

import multiprocessing
from pathlib import Path
from flask import Flask, request, Response, stream_with_context
from flask_cors import CORS
import json
from llama_cpp import Llama

MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q6_K.gguf"
model_file = Path(MODEL_PATH)

if not model_file.is_file():
    print(f"FATAL ERROR: Model file not found at {MODEL_PATH}")
    print("Please download the model and place it in the correct directory.")
    llm = None
else:
    print(f"Loading model from: {model_file}")
    print(f"Using {multiprocessing.cpu_count()} threads, context size 8192, batch size 512")
    try:
        llm = Llama(
            model_path=str(model_file),
            n_ctx=8192,  # The max sequence length to use
            n_threads=multiprocessing.cpu_count(), # Number of CPU threads to use
            n_batch=512,  # Should be between 1 and n_ctx
            use_mlock=True, # Use mlock to keep model in memory
            use_mmap=True,  # Use mmap for faster loads
            verbose=False # Set to True for more detailed Llama.cpp logging
        )
    except Exception as e:
        print(f"Error loading Llama model: {e}")
        llm = None

# --- System Prompt for Redaction ---
SYSTEM_PROMPT = """Remove ALL sensitive information while keeping the text useful for AI assistance. This is for corporate use - strip anything that could identify people, companies, or leak business intelligence.
ALWAYS REPLACE WITH RANGES/GENERICS:
- ALL personal names → Person A, Person B, Person C be consistent
- ALL company names → Company X Company Y Client Corp etc
- ALL email addresses → contact@example.com client@example.com
- ALL phone numbers → 555-0000
- Exact dollar amounts → proper ranges
- Project names → Project Alpha Project Beta
- System names → System A Database X
- Account numbers IDs → [ACCOUNT] [ID-123]
- Dates → [DATE] next week Q4
- Titles with names → Person C Role etc
REDACT:
- Client names and identifiers
- Internal team department names
- Proprietary platform names
- IPs URLs physical addresses
- Contract ticket deal numbers
- Vendor names and internal process names
PRESERVE:
- Technical metrics
- Business requirements
- Job roles titles
- General cities or timing
- Industry-standard concepts
Consistency readability and privacy"""

# --- Flask Application ---
app = Flask(__name__)
CORS(app) # Enable Cross-Origin Resource Sharing

@app.route('/prompt', methods=['POST'])
def handle_prompt():
    if not llm:
        def error_stream():
            yield "data: " + json.dumps({"error": "Model is not loaded or failed to load."}) + "\n\n"
        return Response(stream_with_context(error_stream()), mimetype='text/event-stream')

    data = request.json
    prompt = data.get('prompt', '').strip()
    print(f"Received prompt: '{prompt[:100]}...'")

    if not prompt:
        def error_stream():
            yield "data: " + json.dumps({"error": "No prompt provided."}) + "\n\n"
        return Response(stream_with_context(error_stream()), mimetype='text/event-stream')

    messages = [
        {'role': 'system', 'content': SYSTEM_PROMPT},
        {'role': 'user', 'content': prompt}
    ]

    #A generator function that yields tokens as they are generated by the LLM.
    def generate():
        try:
            stream = llm.create_chat_completion(
                messages=messages,
                max_tokens=2048,
                stop=["<|im_end|>", "</s>", "<|user|>", "You:"], # Common stop tokens
                temperature=0.7,
                stream=True,
            )

            # Iterate over the stream and yield each token's content
            for output in stream:
                if 'choices' in output and len(output['choices']) > 0:
                    delta = output['choices'][0].get('delta', {})
                    token = delta.get('content')
                    if token:
                        yield "data: " + json.dumps({"token": token}) + "\n\n"

        except Exception as e:
            print(f"Inference Error: {e}")
            yield "data: " + json.dumps({"error": str(e)}) + "\n\n"

    # Return a streaming response
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, threaded=True)
