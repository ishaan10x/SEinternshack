import os
os.environ['LLAMA_SET_ROWS'] = '1'

import multiprocessing
from pathlib import Path
from flask import Flask, request, Response, stream_with_context
from flask_cors import CORS
import json
from llama_cpp import Llama

MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q6_K.gguf"
model_file = Path(MODEL_PATH)

if not model_file.is_file():
    print(f"FATAL ERROR: Model file not found at {MODEL_PATH}")
    print("Please download the model and place it in the correct directory.")
    llm = None
else:
    print(f"Loading model from: {model_file}")
    print(f"Using {multiprocessing.cpu_count()} threads, context size 8192, batch size 512")
    try:
        llm = Llama(
            model_path=str(model_file),
            n_ctx=8192,  # The max sequence length to use
            n_threads=multiprocessing.cpu_count(), # Number of CPU threads to use
            n_batch=512,  # Should be between 1 and n_ctx
            use_mlock=True, # Use mlock to keep model in memory
            use_mmap=True,  # Use mmap for faster loads
            verbose=False # Set to True for more detailed Llama.cpp logging
        )
    except Exception as e:
        print(f"Error loading Llama model: {e}")
        llm = None

# --- System Prompt for Redaction ---
name_prompt=f"""Replace ONLY person names with different realistic names. Don't change anything else.
Examples:
- Marcus Thompson → Emma Davis
- Sarah Chen → Lisa Johnson
- David Rodriguez → Ryan Wilson
Text: {text}
Text with names replaced:"""
companies_prompt=f"""Replace ONLY company names with appropriate business descriptions. Don't change anything else.
Examples:
- Goldman Sachs → our financial services client
- Microsoft → the tech company
- Pfizer → our pharmaceutical client
Text: {text}
Text with companies replaced:"""
emails_prompt=f"""Replace ONLY email addresses with generic emails. Don't change anything else.
Examples:
- m.thompson@gs.com → contact@example.com
- s.chen@microsoft.com → team@example.com
- Any email → contact@example.com, info@example.com
Text: {text}
Text with emails replaced:"""
numbers_prompt=f"""Replace ONLY phone numbers and money amounts. Don't change anything else.
Examples:
- 555-987-6543 → [PHONE REDACTED]
- $2.5M → [AMOUNT REDACTED]
- ext 4421 → ext [REDACTED]"""
# --- Flask Application ---
app = Flask(__name__)
CORS(app) # Enable Cross-Origin Resource Sharing

@app.route('/prompt', methods=['POST'])
def handle_prompt():
    if not llm:
        def error_stream():
            yield "data: " + json.dumps({"error": "Model is not loaded or failed to load."}) + "\n\n"
        return Response(stream_with_context(error_stream()), mimetype='text/event-stream')

    data = request.json
    prompt = data.get('prompt', '').strip()
    print(f"Received prompt: '{prompt[:100]}...'")

    if not prompt:
        def error_stream():
            yield "data: " + json.dumps({"error": "No prompt provided."}) + "\n\n"
        return Response(stream_with_context(error_stream()), mimetype='text/event-stream')

    system_prompts = data.get('system_prompts', [name_prompt, companies_prompt, emails_prompt,numbers_prompt])
    if isinstance(system_prompts, str):
        # Allow passing a single string instead of a list
        system_prompts = [system_prompts]

    #A generator function that yields tokens as they are generated by the LLM.
    def generate():
        current_text = prompt
        for idx, sys_prompt in enumerate(system_prompts, start=1):
            print(f"Pass {idx}/{len(system_prompts)} with system prompt:\n{sys_prompt[:80]}...")
            # Combine system + user prompt
            messages = [
                {'role': 'system', 'content': sys_prompt},
                {'role': 'user', 'content': current_text}
            ]
            try:
                # Collect the full result for this pass
                pass_result = ""

                # Notify client that a new pass is starting
                yield "data: " + json.dumps({
                    "pass": idx,
                    "system_prompt": sys_prompt,
                    "start": True
                }) + "\n\n"

                stream = llm.create_chat_completion(
                    messages=messages,
                    max_tokens=2048,
                    stop=["<|im_end|>", "</s>", "<|user|>", "You:"],
                    temperature=0.7,
                    stream=True,
                )

                for output in stream:
                    if 'choices' in output and len(output['choices']) > 0:
                        delta = output['choices'][0].get('delta', {})
                        token = delta.get('content')
                        if token:
                            pass_result += token
                            yield "data: " + json.dumps({
                                "pass": idx,
                                "token": token
                            }) + "\n\n"

                # Notify client that this pass is done
                yield "data: " + json.dumps({
                    "pass": idx,
                    "done": True
                }) + "\n\n"

                # The output of this pass becomes the input for the next
                current_text = pass_result.strip()

            except Exception as e:
                print(f"Inference Error at pass {idx}: {e}")
                yield "data: " + json.dumps({
                    "pass": idx,
                    "error": str(e)
                }) + "\n\n"
                break  # stop the chain if an error occurs

        # Return a streaming response
        return Response(stream_with_context(generate()), mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, threaded=True)
